{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark, pyspark\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "file_attack = 'gtd_14to17_0718dist'\n",
    "file_airport = 'airports-extended'\n",
    "file_largest_airport = 'largest-global-airports-passenger-traffic'\n",
    "file_passengers = 'avia_par_be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = 'D:/DataMining/taba/'\n",
    "data_path = '../data/Datasets/'\n",
    "\n",
    "attack_data = sc.textFile(data_path + 'gtd-data/' + file_attack + '.csv')\n",
    "airport_data = sc.textFile(data_path + file_airport + '.csv')\n",
    "largest_airport_data = sc.textFile(data_path + file_largest_airport + '.xlsx')\n",
    "passenger_data = sc.textFile(data_path + file_passengers + '.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALT WAY DOES NOT WORK YET QUESTION\n",
    "#df2 = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load(data_path + 'gtd-data/' + file_attack + '.csv')\n",
    "#df2.printSchema()\n",
    "#df2.select(df2.country_txt).show(1)\n",
    "#df2.select(col('country_txt')).show(1)\n",
    "# https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "# => fill null values in dataframe -> fillna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Attack Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for dataframe\n",
    "new_columns = attack_data.first()\n",
    "new_columns = new_columns.split(\";\")\n",
    "attack_data = attack_data.filter(lambda l: l != new_columns)\n",
    "df_terror_data = attack_data.map(lambda x: x.split(';')).toDF()\n",
    "old_columns = df_terror_data.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply new column names\n",
    "df_terror_data = reduce(lambda data, idx: data.withColumnRenamed(old_columns[idx], new_columns[idx]), range(len(old_columns)), df_terror_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Attack Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_result = df_terror_data.filter(col(\"country_txt\") == \"Belgium\").select('iyear', 'imonth', 'iday','country_txt', 'summary')\n",
    "query_result = df_terror_data.filter(df_terror_data.country_txt == \"Belgium\").select('iyear', 'imonth', 'iday','country_txt', 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+-----------+--------------------+\n",
      "|iyear|imonth|iday|country_txt|             summary|\n",
      "+-----+------+----+-----------+--------------------+\n",
      "| 2014|     5|  24|    Belgium|05/24/2014: Assai...|\n",
      "| 2014|     9|  16|    Belgium|09/16/2014: Assai...|\n",
      "| 2016|     3|  22|    Belgium|03/22/2016: Two s...|\n",
      "| 2016|     3|  22|    Belgium|03/22/2016: A sui...|\n",
      "| 2016|     8|   6|    Belgium|08/06/2016: An as...|\n",
      "| 2016|     8|  29|    Belgium|08/29/2016: Assai...|\n",
      "| 2016|    10|   5|    Belgium|10/05/2016: An as...|\n",
      "| 2016|    12|  23|    Belgium|12/23/2016: Secur...|\n",
      "| 2017|     5|   1|    Belgium|\"05/01/2017: An a...|\n",
      "| 2017|     6|  20|    Belgium|\"06/20/2017: A su...|\n",
      "+-----+------+----+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|05/24/2014: Assailants opened fire on visitors of the Jewish Museum in Brussels city, Brussels capital region, Belgium. At least four people, including two Israeli tourists, a French tourist, and a Belgian museum worker, were killed in the assault. Mehdi Nenmouche, an Islamic State of Iraq and Levant (ISIL) member who had recently returned from Syria, claimed responsibility for the attack.                                                                                                                                                                                                                                                                                                                                               |\n",
      "|09/16/2014: Assailants set a synagogue on fire in Anderlecht neighborhood, Brussels city, Brussels region, Belgium. Three people were injured in the attack. No group claimed responsibility for the incident.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|03/22/2016: Two suicide bombers with explosives-laden suitcases detonated at a check-in counter at Brussels Airport in Zaventem, Flemish Brabant, Belgium. In addition to the two bombers, at least 16 people were killed, including four United States citizens, in the blasts. A third explosive device was discovered and defused by security following the incident. This was one of two coordinated attacks targeting transportation infrastructure in Brussels on the same day. Additionally, at least 270 people were injured across both incidents. The Islamic State of Iraq and the Levant (ISIL) claimed responsibility and stated that the attacks were carried out in retaliation for Belgium's participation in a coalition against ISIL.|\n",
      "|03/22/2016: A suicide bomber detonated at the Maalbeek Metro Station in Brussels, Belgium. In addition to the bomber, at least 16 people were killed in the blast. This was one of two coordinated attacks targeting transportation infrastructure in Brussels on the same day. Additionally, at least 270 people were injured across both incidents. The Islamic State of Iraq and the Levant (ISIL) claimed responsibility and stated that the attacks were carried out in retaliation for Belgium's participation in a coalition against ISIL.                                                                                                                                                                                                      |\n",
      "|08/06/2016: An assailant armed with a machete attacked police officers near the police headquarters in Charleroi, Wallonia, Belgium. Two police officers were injured and the assailant was killed when a third officer opened fire. Authorities identified the assailant as Khaled Babbouri. The Islamic State of Iraq and the Levant (ISIL) claimed responsibility for the incident. however, Babbouri's connection to ISIL could not be confirmed.                                                                                                                                                                                                                                                                                                  |\n",
      "|08/29/2016: Assailants drove a vehicle through the front gate of the Brussels National Institute of Criminology laboratory and then set the vehicle on fire in Brussels, Belgium. There were no reported casualties in the attack. No group claimed responsibility for the incident.                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|10/05/2016: An assailant stabbed police officers in Schaerbeek, Brussels, Belgium. Three police officers and the assailant were injured in the attack and ensuing police pursuit. A Muslim extremist, identified as Hicham Diop, claimed responsibility for the incident and stated that he carried out the attack in retaliation for police running him over in 2011.                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|12/23/2016: Security forces discovered and defused an explosive device outside of a Turkish community center in Brussels, Belgium. No group claimed responsibility for the incident.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|\"05/01/2017: An assailant stabbed and injured Raoul Hedebouw, a member of Parliament, at a Labour day rally in Liege, Wallonia, Belgium. Police arrested an unnamed individual in connection with the attack. The assailant allegedly shouted \"\"These PTB guys are idiots\"\" during the attack.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|\"06/20/2017: A suicide bomber attempted to detonate an explosives-laden suitcase at the Brussels Central Station in Brussels, Belgium. The suitcase partially detonated twice without causing any casualties or damage. Following the failed blasts, the assailant charged soldiers while shouting \"\"Allahu Akbar\"\" before he was shot and killed. No group claimed responsibility for the incident. however, sources identified the assailant as Oussama Zariouh and stated that he was sympathetic to the Islamic State of Iraq and the Levant (ISIL).\"                                                                                                                                                                                              |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_result.select('summary').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare airport_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 'Goroka Airport', 'Goroka', 'Papua New Guinea', 'GKA', 'AYGA', -6.08168983459, 145.391998291, 5282, '10', 'U', 'Pacific/Port_Moresby', 'airport', 'OurAirports'], [2, 'Madang Airport', 'Madang', 'Papua New Guinea', 'MAG', 'AYMD', -5.20707988739, 145.789001465, 20, '10', 'U', 'Pacific/Port_Moresby', 'airport', 'OurAirports'], [3, 'Mount Hagen Kagamuga Airport', 'Mount Hagen', 'Papua New Guinea', 'HGU', 'AYMH', -5.82678985595703, 144.296005249023, 5388, '10', 'U', 'Pacific/Port_Moresby', 'airport', 'OurAirports'], [4, 'Nadzab Airport', 'Nadzab', 'Papua New Guinea', 'LAE', 'AYNZ', -6.569803, 146.725977, 239, '10', 'U', 'Pacific/Port_Moresby', 'airport', 'OurAirports']]\n"
     ]
    }
   ],
   "source": [
    "fields = []\n",
    "fields.append(StructField('airport_id', IntegerType(), True)) \n",
    "fields.append(StructField('airport_name', StringType(), True))\n",
    "fields.append(StructField('city', StringType(), True))\n",
    "fields.append(StructField('country', StringType(), True))\n",
    "fields.append(StructField('IATA', StringType(), True)) # kind of airport identifier\n",
    "fields.append(StructField('ICAO', StringType(), True)) # different kind of airport identifier\n",
    "fields.append(StructField('latitude', FloatType(), True))\n",
    "fields.append(StructField('longitude', FloatType(), True))\n",
    "fields.append(StructField('unknown1', IntegerType(), True))\n",
    "fields.append(StructField('unknown2', StringType(), True))\n",
    "fields.append(StructField('unknown3', StringType(), True))\n",
    "fields.append(StructField('continent_city', StringType(), True))\n",
    "fields.append(StructField('idk_column', StringType(), True))\n",
    "fields.append(StructField('source', StringType(), True))\n",
    "schema = StructType(fields)\n",
    "\n",
    "clean_data = (airport_data\n",
    "              .map(lambda line: line.split(';'))\n",
    "              .map(lambda line: [int(line[0]),line[1],line[2],line[3],line[4],line[5],float(line[6]),float(line[7]),int(line[8]),line[9],line[10],line[11],line[12],line[13]])\n",
    "             )\n",
    "df_airport_data = sqlContext.createDataFrame(clean_data, schema)\n",
    "#df_airport_data = sqlContext.createDataFrame(airport_data.map(lambda line: line.split(';')), schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query airport_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------+---------------+\n",
      "|airport_name                          |city             |continent_city |\n",
      "+--------------------------------------+-----------------+---------------+\n",
      "|Antwerp International Airport (Deurne)|Antwerp          |Europe/Brussels|\n",
      "|Beauvechain Air Base                  |Beauvechain      |Europe/Brussels|\n",
      "|Kleine Brogel Air Base                |Kleine Brogel    |Europe/Brussels|\n",
      "|Brussels Airport                      |Brussels         |Europe/Brussels|\n",
      "|Jehonville Air Base                   |Bertrix          |Europe/Brussels|\n",
      "|Brussels South Charleroi Airport      |Charleroi        |Europe/Brussels|\n",
      "|Chièvres Air Base                     |Chievres         |Europe/Brussels|\n",
      "|Koksijde Air Base                     |Koksijde         |Europe/Brussels|\n",
      "|Florennes Air Base                    |Florennes        |Europe/Brussels|\n",
      "|Wevelgem Airport                      |Kortrijk-vevelgem|Europe/Brussels|\n",
      "|Liège Airport                         |Liege            |Europe/Brussels|\n",
      "|Ostend-Bruges International Airport   |Ostend           |Europe/Brussels|\n",
      "|Zutendaal Air Base                    |Zutendaal        |Europe/Brussels|\n",
      "|Limburg Regional Airport              |Sint-truiden     |Europe/Brussels|\n",
      "|Saint Hubert Air Base                 |St.-hubert       |Europe/Brussels|\n",
      "|Ursel Air Base                        |Ursel            |Europe/Brussels|\n",
      "|Weelde Air Base                       |Weelde           |Europe/Brussels|\n",
      "|Oostmalle Air Base                    |Zoersel          |Europe/Brussels|\n",
      "|Brussels Gare du Midi                 |Brussels         |Europe/Brussels|\n",
      "|Engels heliport                       |Ebenhofen        |Europe/Brussels|\n",
      "|Westkapelle heliport                  |Knokke           |Europe/Brussels|\n",
      "|Gent Sint-Pieters                     |Gent             |Europe/Brussels|\n",
      "|Brugge                                |Bruges           |Europe/Brussels|\n",
      "|Brugge                                |Bruges           |Europe/Brussels|\n",
      "|Schaffen Diest                        |Schaffen         |Europe/Brussels|\n",
      "|HOEVENEN                              |HOEVENEN         |Europe/Brussels|\n",
      "|Centraal                              |Antwerp          |Europe/Brussels|\n",
      "|Liege-Guillemins Railway Station      |Liege            |Europe/Brussels|\n",
      "|Bruxelles-Central                     |Brussels         |Europe/Brussels|\n",
      "|Goetsenhoven Air Base                 |GOETSENHOVEN     |Europe/Brussels|\n",
      "|Spa (la Sauvenière) Airport           |Spa              |Europe/Brussels|\n",
      "|Suarlée Airport                       |Namur            |Europe/Brussels|\n",
      "|Kiewit Airfield Hasselt               |Hasselt          |Europe/Brussels|\n",
      "|Brussels Gare Centrale                |Brussels         |Europe/Brussels|\n",
      "|Eupen - Rail                          |Eupen            |Europe/Brussels|\n",
      "|Brussels Nord                         |Brussels         |Europe/Brussels|\n",
      "|Brugge Railway Station                |Bruges           |Europe/Brussels|\n",
      "|Gare de Coo                           |Coo              |Europe/Brussels|\n",
      "|Knokke Station                        |Knokke           |Europe/Brussels|\n",
      "|Kortrijk Station                      |Kortrijk         |Europe/Brussels|\n",
      "|Ieper Station                         |Ypres            |Europe/Brussels|\n",
      "|Gare de Dinant                        |Dinant           |Europe/Brussels|\n",
      "|Waterloo Station                      |Waterloo         |Europe/Brussels|\n",
      "+--------------------------------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df_airport_data\n",
    " .select(df_airport_data.airport_name, df_airport_data.city,df_airport_data.continent_city)\n",
    " .filter(df_airport_data.continent_city == 'Europe/Brussels')\n",
    " .show(df_airport_data.count(),False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare passenger data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "international_passengers_file =  'international-passengers-'\n",
    "international_passengers_2013 = sc.textFile(data_path + international_passengers_file + '2013' + '.csv')\n",
    "international_passengers_2014 = sc.textFile(data_path + international_passengers_file + '2014' + '.csv')\n",
    "#international_passengers_2015 = sc.textFile(data_path + international_passengers_file + '2015' + '.csv')\n",
    "\n",
    "total_passengers_file = 'total-passengers-'\n",
    "total_passengers_2013 = sc.textFile(data_path + total_passengers_file + '2013' + '.csv')\n",
    "total_passengers_2014 = sc.textFile(data_path + total_passengers_file + '2014' + '.csv')\n",
    "#total_passengers_2015 = sc.textFile(data_path + total_passengers_file + '2015' + '.csv')\n",
    "\n",
    "def clean_passenger_data(rdd):\n",
    "    \n",
    "    rdd = rdd.map(lambda l: l.split(\";\"))\n",
    "    \n",
    "\n",
    "    new_columns = rdd.map(lambda l: l[3].split(\"/\"))\n",
    "\n",
    "    rdd = rdd.map(lambda l: l[0:3] + l[4:])\n",
    "\n",
    "    rdd = rdd.zip(new_columns)\n",
    "\n",
    "    rdd = rdd.map(lambda l: l[0] + l[1])\n",
    "    \n",
    "    \n",
    "    print(rdd.take(2))\n",
    "    print()\n",
    "    print(\"+++++++++++++++++++++++++++++++++\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Rank', 'Airport', 'Location', 'Total passengers', '% change', 'IATA', 'ICAO'], ['1', 'London Heathrow Airport', 'Hillingdon, Greater London, United Kingdom', '66689466', '4%', 'LHR', 'EGLL']]\n",
      "\n",
      "+++++++++++++++++++++++++++++++++\n",
      "\n",
      "[['Rank', 'Airport', 'Location', 'Total passengers', '% change', 'IATA', 'ICAO'], ['1', 'Dubai International Airport', 'Garhoud, Dubai, United Arab Emirates', '69954392', '6.2%', 'DXB', 'OMDB']]\n",
      "\n",
      "+++++++++++++++++++++++++++++++++\n",
      "\n",
      "[['Rank', 'Airport', 'Location', ' Total passengers ', '% change', 'IATA', 'ICAO'], ['1', 'Hartsfield–Jackson Atlanta International Airport', 'Atlanta, Georgia, United States', '94430785', '1%', 'ATL', 'KATL']]\n",
      "\n",
      "+++++++++++++++++++++++++++++++++\n",
      "\n",
      "[['Rank', 'Airport', 'Location', 'IATA/ICAO', 'Total passengers', '% change', 'Country'], ['1', 'Hartsfield–Jackson Atlanta International Airport', 'Atlanta, Georgia', 'ATL/KATL', '96178899', '1.9%', 'United States']]\n",
      "\n",
      "+++++++++++++++++++++++++++++++++\n",
      "\n",
      "['Rank;Airport;Location;IATA/ICAO;Total passengers;% change', '1;London Heathrow Airport;Hillingdon, Greater London, United Kingdom;LHR/EGLL;66689466;4%']\n"
     ]
    }
   ],
   "source": [
    "all_passenger_datasets_except2015 = [international_passengers_2013,international_passengers_2014,total_passengers_2013, total_passengers_2014]\n",
    "\n",
    "for dataset in all_passenger_datasets_except2015:\n",
    "    clean_passenger_data(dataset)\n",
    "print(international_passengers_2013.take(2)) # QUESTION no pointers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix the data from 2015 missing ICAO column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = []\n",
    "fields.append(StructField('rank', StringType(), True))\n",
    "fields.append(StructField('airport_name', StringType(), True))\n",
    "fields.append(StructField('location', StringType(), True))\n",
    "fields.append(StructField('country', StringType(), True))\n",
    "fields.append(StructField('total_passengers', StringType(), True))\n",
    "fields.append(StructField('change', StringType(), True))\n",
    "fields.append(StructField('IATA', StringType(), True))\n",
    "schema_total_passengers_2015 = StructType(fields)\n",
    "\n",
    "fields = []\n",
    "fields.append(StructField('rank', StringType(), True))\n",
    "fields.append(StructField('airport_name', StringType(), True))\n",
    "fields.append(StructField('location', StringType(), True))\n",
    "fields.append(StructField('total_passengers', StringType(), True))\n",
    "fields.append(StructField('change', StringType(), True))\n",
    "fields.append(StructField('IATA', StringType(), True))\n",
    "schema_international_passengers_2015 = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total passenger data 2015\n",
      "+----+----+--------------------+---------+-------+----------------+------+----+\n",
      "|IATA|rank|        airport_name| location|country|total_passengers|change|ICAO|\n",
      "+----+----+--------------------+---------+-------+----------------+------+----+\n",
      "| FRA|  12|Flughafen Frankfu...|Frankfurt|Germany|        61032022|   2.5|EDDF|\n",
      "| IST|  11|Atatürk Internati...| Istanbul| Turkey|        61346229|   8.2|LTBA|\n",
      "+----+----+--------------------+---------+-------+----------------+------+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++++++++\n",
      "\n",
      "international passenger data 2015\n",
      "+----+----+--------------------+-----------------+----------------+------+----+\n",
      "|IATA|rank|        airport_name|         location|total_passengers|change|ICAO|\n",
      "+----+----+--------------------+-----------------+----------------+------+----+\n",
      "| PMI|  36|Aeropuerto de Pal...|Palma De Mallorca|        18107070|   0.6|LEPA|\n",
      "| HEL|  48|Helsinki-Vantaa A...|         Helsinki|        13826868|   2.9|EFHK|\n",
      "+----+----+--------------------+-----------------+----------------+------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_passengers_2015 = sc.textFile(data_path + total_passengers_file + '2015' + '.csv')\n",
    "international_passengers_2015 = sc.textFile(data_path + international_passengers_file + '2015' + '.csv')\n",
    "\n",
    "\n",
    "# remove header\n",
    "header = total_passengers_2015.first()\n",
    "total_passengers_2015 = total_passengers_2015.filter(lambda l: l != header)\n",
    "# create dataframe\n",
    "df_total_passengers_2015 = sqlContext.createDataFrame(total_passengers_2015.map(lambda l: l.split(\";\")),schema_total_passengers_2015)\n",
    "# add missing column by join using previous data (df_airport_data)\n",
    "df_total_passengers_2015 = df_total_passengers_2015.join(df_airport_data.select(df_airport_data.IATA, df_airport_data.ICAO),\"IATA\")\n",
    "\n",
    "# repeat for second dataset from 2015\n",
    "international_passengers_2015 = sc.textFile(data_path + international_passengers_file + '2015' + '.csv')\n",
    "header = international_passengers_2015.first()\n",
    "international_passengers_2015 = international_passengers_2015.filter(lambda l: l != header)\n",
    "temp = international_passengers_2015.map(lambda l: l.split(\";\"))\n",
    "df_international_passengers_2015 = sqlContext.createDataFrame(international_passengers_2015.map(lambda l: l.split(\";\")),schema_international_passengers_2015)\n",
    "df_international_passengers_2015 = df_international_passengers_2015.join(df_airport_data.select(df_airport_data.IATA, df_airport_data.ICAO),\"IATA\")\n",
    "\n",
    "# print results\n",
    "print(\"total passenger data 2015\")\n",
    "df_total_passengers_2015.show(2)\n",
    "print()\n",
    "print(\"++++++++++++++++++++++++++++++\")\n",
    "print()\n",
    "print(\"international passenger data 2015\")\n",
    "df_international_passengers_2015.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make the dataframes for the other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Rank', 'Airport', 'Location', 'IATA/ICAO', 'Total passengers', '% change'], ['1', 'London Heathrow Airport', 'Hillingdon, Greater London, United Kingdom', 'LHR/EGLL', '66689466', '4%']]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o10850.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 650.0 failed 1 times, most recent failure: Lost task 0.0 in stage 650.0 (TID 1155, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/spark/python/pyspark/sql/session.py\", line 673, in prepare\n    verify_func(obj)\n  File \"/opt/spark/python/pyspark/sql/types.py\", line 1421, in verify\n    verify_value(obj)\n  File \"/opt/spark/python/pyspark/sql/types.py\", line 1400, in verify_struct\n    \"length of fields (%d)\" % (len(obj), len(verifiers))))\nValueError: Length of object (6) does not match with length of fields (7)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/spark/python/pyspark/sql/session.py\", line 673, in prepare\n    verify_func(obj)\n  File \"/opt/spark/python/pyspark/sql/types.py\", line 1421, in verify\n    verify_value(obj)\n  File \"/opt/spark/python/pyspark/sql/types.py\", line 1400, in verify_struct\n    \"length of fields (%d)\" % (len(obj), len(verifiers))))\nValueError: Length of object (6) does not match with length of fields (7)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-342-670f975d8944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf_international_passengers_2013\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_international_passengers_2014\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_total_passengers_2013\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_total_passengers_2014\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"+++++++++++++++++++++++++++++++++\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o10850.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 650.0 failed 1 times, most recent failure: Lost task 0.0 in stage 650.0 (TID 1155, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/spark/python/pyspark/sql/session.py\", line 673, in prepare\n    verify_func(obj)\n  File \"/opt/spark/python/pyspark/sql/types.py\", line 1421, in verify\n    verify_value(obj)\n  File \"/opt/spark/python/pyspark/sql/types.py\", line 1400, in verify_struct\n    \"length of fields (%d)\" % (len(obj), len(verifiers))))\nValueError: Length of object (6) does not match with length of fields (7)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/spark/python/pyspark/sql/session.py\", line 673, in prepare\n    verify_func(obj)\n  File \"/opt/spark/python/pyspark/sql/types.py\", line 1421, in verify\n    verify_value(obj)\n  File \"/opt/spark/python/pyspark/sql/types.py\", line 1400, in verify_struct\n    \"length of fields (%d)\" % (len(obj), len(verifiers))))\nValueError: Length of object (6) does not match with length of fields (7)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "fields = []\n",
    "fields.append(StructField('rank', StringType(), True))\n",
    "fields.append(StructField('airport_name', StringType(), True))\n",
    "fields.append(StructField('location', StringType(), True))\n",
    "fields.append(StructField('total_passengers', StringType(), True))\n",
    "fields.append(StructField('change', StringType(), True))\n",
    "fields.append(StructField('IATA', StringType(), True))\n",
    "fields.append(StructField('ICAO', StringType(), True))\n",
    "schema_passengers_data = StructType(fields)\n",
    "\n",
    "temp = international_passengers_2013.map(lambda l: l.split(\";\"))\n",
    "print(temp.take(2))\n",
    "df_international_passengers_2013 = sqlContext.createDataFrame(international_passengers_2013.map(lambda l: l.split(\";\")), schema_passengers_data)\n",
    "df_international_passengers_2014 = sqlContext.createDataFrame(international_passengers_2014.map(lambda l: l.split(\";\")), schema_passengers_data)\n",
    "df_total_passengers_2013 = sqlContext.createDataFrame(total_passengers_2013.map(lambda l: l.split(\";\")), schema_passengers_data)\n",
    "df_total_passengers_2014 = sqlContext.createDataFrame(total_passengers_2014.map(lambda l: l.split(\";\")), schema_passengers_data)\n",
    "\n",
    "\n",
    "datasets = [df_international_passengers_2013,df_international_passengers_2014,df_total_passengers_2013,df_total_passengers_2014]\n",
    "for df in datasets:\n",
    "    print(df.show(2))\n",
    "    print()\n",
    "    print(\"+++++++++++++++++++++++++++++++++\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare passenger_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
